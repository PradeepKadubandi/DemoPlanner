{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks import AutoEncoder\n",
    "from dataset import NumpyCsvDataSet\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = np.get_printoptions()\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 1026])\n"
     ]
    }
   ],
   "source": [
    "data_folder = 'data/img_auto_encoder/version2'\n",
    "train_data = NumpyCsvDataSet(data_folder + '/train.csv')\n",
    "test_data = NumpyCsvDataSet(data_folder + '/test.csv')\n",
    "training_loader = DataLoader(train_data, batch_size=50, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=3, shuffle=True)\n",
    "print (train_data.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dim = 2\n",
    "img_res = 32\n",
    "img_size = img_res * img_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't know how to reset  autoencoder, please run `%reset?` for details\n",
      "Don't know how to reset  net, please run `%reset?` for details\n"
     ]
    }
   ],
   "source": [
    "%reset -f AutoEncoder\n",
    "%reset -f net\n",
    "from networks import AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoEncoder(\n",
       "  (encoder): Sequential(\n",
       "    (enc_conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (enc_relu1): ReLU()\n",
       "    (enc_conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (enc_relu2): ReLU()\n",
       "    (enc_conv3): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (enc_relu3): ReLU()\n",
       "    (enc_reshape1): Reshape()\n",
       "    (enc_fc1): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (dec_fc1): Linear(in_features=2, out_features=128, bias=True)\n",
       "    (dec_relu4): ReLU()\n",
       "    (dec_reshape1): Reshape()\n",
       "    (dec_convt3): ConvTranspose2d(8, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (dec_relu3): ReLU()\n",
       "    (dec_convt2): ConvTranspose2d(4, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (dec_relu2): ReLU()\n",
       "    (dec_conv1): ConvTranspose2d(4, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (dec_sigmoid1): Sigmoid()\n",
       "    (dec_flat1): Flatten()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = AutoEncoder()\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training-AutoEncoder-01-22-16-47-37\n"
     ]
    }
   ],
   "source": [
    "expt_prefix = 'Training-AutoEncoder-'\n",
    "expt_name = expt_prefix + time.strftime('%m-%d-%H-%M-%S')\n",
    "print (expt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expt_name = 'Training-AutoEncoder-01-18-16-55-55'\n",
    "folder = './saved_model/img_auto_encoder/' + expt_name\n",
    "PATH = folder + '/autoenc.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "        Network Parameter Statistics\n",
      "--------------------------------------------\n",
      "Params for layer 1 = 36\n",
      "Params for layer 2 = 4\n",
      "Params for layer 3 = 144\n",
      "Params for layer 4 = 4\n",
      "Params for layer 5 = 288\n",
      "Params for layer 6 = 8\n",
      "Params for layer 7 = 256\n",
      "Params for layer 8 = 2\n",
      "Params for layer 9 = 256\n",
      "Params for layer 10 = 128\n",
      "Params for layer 11 = 288\n",
      "Params for layer 12 = 4\n",
      "Params for layer 13 = 144\n",
      "Params for layer 14 = 4\n",
      "Params for layer 15 = 36\n",
      "Params for layer 16 = 1\n",
      "--------------------------------------------\n",
      "Total: 1603\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AutoEncoder(\n",
       "  (encoder): Sequential(\n",
       "    (enc_conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (enc_relu1): ReLU()\n",
       "    (enc_conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (enc_relu2): ReLU()\n",
       "    (enc_conv3): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (enc_relu3): ReLU()\n",
       "    (enc_reshape1): Reshape()\n",
       "    (enc_fc1): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (dec_fc1): Linear(in_features=2, out_features=128, bias=True)\n",
       "    (dec_relu4): ReLU()\n",
       "    (dec_reshape1): Reshape()\n",
       "    (dec_convt3): ConvTranspose2d(8, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (dec_relu3): ReLU()\n",
       "    (dec_convt2): ConvTranspose2d(4, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (dec_relu2): ReLU()\n",
       "    (dec_conv1): ConvTranspose2d(4, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (dec_sigmoid1): Sigmoid()\n",
       "    (dec_flat1): Flatten()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('--------------------------------------------')\n",
    "print ('        Network Parameter Statistics')\n",
    "print ('--------------------------------------------')\n",
    "total = 0\n",
    "layer = 0\n",
    "for p in net.parameters():\n",
    "    if p.requires_grad:\n",
    "        total += p.numel()\n",
    "        print ('Params for layer {} = {}'.format(layer+1, p.numel()))\n",
    "        layer += 1\n",
    "print ('--------------------------------------------')\n",
    "print ('Total: {}'.format(total))\n",
    "print ('--------------------------------------------')\n",
    "# trainable_params_count = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "# print ('Total Number of parameters = {}'.format(trainable_params_count))\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[201,   100] loss: 0.018\n",
      "[201,   200] loss: 0.018\n",
      "[202,   100] loss: 0.018\n",
      "[202,   200] loss: 0.018\n",
      "[203,   100] loss: 0.018\n",
      "[203,   200] loss: 0.018\n",
      "[204,   100] loss: 0.018\n",
      "[204,   200] loss: 0.018\n",
      "[205,   100] loss: 0.018\n",
      "[205,   200] loss: 0.018\n",
      "[206,   100] loss: 0.018\n",
      "[206,   200] loss: 0.018\n",
      "[207,   100] loss: 0.018\n",
      "[207,   200] loss: 0.018\n",
      "[208,   100] loss: 0.018\n",
      "[208,   200] loss: 0.018\n",
      "[209,   100] loss: 0.018\n",
      "[209,   200] loss: 0.018\n",
      "[210,   100] loss: 0.018\n",
      "[210,   200] loss: 0.018\n",
      "[211,   100] loss: 0.018\n",
      "[211,   200] loss: 0.018\n",
      "[212,   100] loss: 0.018\n",
      "[212,   200] loss: 0.018\n",
      "[213,   100] loss: 0.018\n",
      "[213,   200] loss: 0.018\n",
      "[214,   100] loss: 0.018\n",
      "[214,   200] loss: 0.018\n",
      "[215,   100] loss: 0.018\n",
      "[215,   200] loss: 0.018\n",
      "[216,   100] loss: 0.018\n",
      "[216,   200] loss: 0.018\n",
      "[217,   100] loss: 0.018\n",
      "[217,   200] loss: 0.018\n",
      "[218,   100] loss: 0.018\n",
      "[218,   200] loss: 0.018\n",
      "[219,   100] loss: 0.018\n",
      "[219,   200] loss: 0.018\n",
      "[220,   100] loss: 0.018\n",
      "[220,   200] loss: 0.018\n",
      "[221,   100] loss: 0.018\n",
      "[221,   200] loss: 0.018\n",
      "[222,   100] loss: 0.018\n",
      "[222,   200] loss: 0.018\n",
      "[223,   100] loss: 0.018\n",
      "[223,   200] loss: 0.018\n",
      "[224,   100] loss: 0.018\n",
      "[224,   200] loss: 0.018\n",
      "[225,   100] loss: 0.018\n",
      "[225,   200] loss: 0.018\n",
      "[226,   100] loss: 0.018\n",
      "[226,   200] loss: 0.018\n",
      "[227,   100] loss: 0.018\n",
      "[227,   200] loss: 0.018\n",
      "[228,   100] loss: 0.018\n",
      "[228,   200] loss: 0.018\n",
      "[229,   100] loss: 0.018\n",
      "[229,   200] loss: 0.018\n",
      "[230,   100] loss: 0.018\n",
      "[230,   200] loss: 0.018\n",
      "[231,   100] loss: 0.018\n",
      "[231,   200] loss: 0.018\n",
      "[232,   100] loss: 0.018\n",
      "[232,   200] loss: 0.018\n",
      "[233,   100] loss: 0.018\n",
      "[233,   200] loss: 0.018\n",
      "[234,   100] loss: 0.018\n",
      "[234,   200] loss: 0.018\n",
      "[235,   100] loss: 0.018\n",
      "[235,   200] loss: 0.018\n",
      "[236,   100] loss: 0.018\n",
      "[236,   200] loss: 0.018\n",
      "[237,   100] loss: 0.018\n",
      "[237,   200] loss: 0.018\n",
      "[238,   100] loss: 0.018\n",
      "[238,   200] loss: 0.018\n",
      "[239,   100] loss: 0.018\n",
      "[239,   200] loss: 0.018\n",
      "[240,   100] loss: 0.018\n",
      "[240,   200] loss: 0.018\n",
      "[241,   100] loss: 0.018\n",
      "[241,   200] loss: 0.018\n",
      "[242,   100] loss: 0.018\n",
      "[242,   200] loss: 0.018\n",
      "[243,   100] loss: 0.018\n",
      "[243,   200] loss: 0.018\n",
      "[244,   100] loss: 0.018\n",
      "[244,   200] loss: 0.018\n",
      "[245,   100] loss: 0.018\n",
      "[245,   200] loss: 0.018\n",
      "[246,   100] loss: 0.018\n",
      "[246,   200] loss: 0.018\n",
      "[247,   100] loss: 0.018\n",
      "[247,   200] loss: 0.018\n",
      "[248,   100] loss: 0.018\n",
      "[248,   200] loss: 0.018\n",
      "[249,   100] loss: 0.018\n",
      "[249,   200] loss: 0.018\n",
      "[250,   100] loss: 0.018\n",
      "[250,   200] loss: 0.018\n",
      "[251,   100] loss: 0.018\n",
      "[251,   200] loss: 0.018\n",
      "[252,   100] loss: 0.018\n",
      "[252,   200] loss: 0.018\n",
      "[253,   100] loss: 0.018\n",
      "[253,   200] loss: 0.018\n",
      "[254,   100] loss: 0.018\n",
      "[254,   200] loss: 0.018\n",
      "[255,   100] loss: 0.018\n",
      "[255,   200] loss: 0.018\n",
      "[256,   100] loss: 0.018\n",
      "[256,   200] loss: 0.018\n",
      "[257,   100] loss: 0.018\n",
      "[257,   200] loss: 0.018\n",
      "[258,   100] loss: 0.018\n",
      "[258,   200] loss: 0.018\n",
      "[259,   100] loss: 0.018\n",
      "[259,   200] loss: 0.018\n",
      "[260,   100] loss: 0.018\n",
      "[260,   200] loss: 0.018\n",
      "[261,   100] loss: 0.018\n",
      "[261,   200] loss: 0.018\n",
      "[262,   100] loss: 0.018\n",
      "[262,   200] loss: 0.018\n",
      "[263,   100] loss: 0.018\n",
      "[263,   200] loss: 0.018\n",
      "[264,   100] loss: 0.018\n",
      "[264,   200] loss: 0.018\n",
      "[265,   100] loss: 0.018\n",
      "[265,   200] loss: 0.018\n",
      "[266,   100] loss: 0.018\n",
      "[266,   200] loss: 0.018\n",
      "[267,   100] loss: 0.018\n",
      "[267,   200] loss: 0.018\n",
      "[268,   100] loss: 0.018\n",
      "[268,   200] loss: 0.018\n",
      "[269,   100] loss: 0.018\n",
      "[269,   200] loss: 0.018\n",
      "[270,   100] loss: 0.018\n",
      "[270,   200] loss: 0.018\n",
      "[271,   100] loss: 0.018\n",
      "[271,   200] loss: 0.018\n",
      "[272,   100] loss: 0.018\n",
      "[272,   200] loss: 0.018\n",
      "[273,   100] loss: 0.018\n",
      "[273,   200] loss: 0.018\n",
      "[274,   100] loss: 0.018\n",
      "[274,   200] loss: 0.018\n",
      "[275,   100] loss: 0.018\n",
      "[275,   200] loss: 0.018\n",
      "[276,   100] loss: 0.018\n",
      "[276,   200] loss: 0.018\n",
      "[277,   100] loss: 0.018\n",
      "[277,   200] loss: 0.018\n",
      "[278,   100] loss: 0.018\n",
      "[278,   200] loss: 0.018\n",
      "[279,   100] loss: 0.018\n",
      "[279,   200] loss: 0.018\n",
      "[280,   100] loss: 0.018\n",
      "[280,   200] loss: 0.018\n",
      "[281,   100] loss: 0.018\n",
      "[281,   200] loss: 0.018\n",
      "[282,   100] loss: 0.018\n",
      "[282,   200] loss: 0.018\n",
      "[283,   100] loss: 0.018\n",
      "[283,   200] loss: 0.018\n",
      "[284,   100] loss: 0.018\n",
      "[284,   200] loss: 0.018\n",
      "[285,   100] loss: 0.018\n",
      "[285,   200] loss: 0.018\n",
      "[286,   100] loss: 0.018\n",
      "[286,   200] loss: 0.018\n",
      "[287,   100] loss: 0.018\n",
      "[287,   200] loss: 0.018\n",
      "[288,   100] loss: 0.018\n",
      "[288,   200] loss: 0.018\n",
      "[289,   100] loss: 0.018\n",
      "[289,   200] loss: 0.018\n",
      "[290,   100] loss: 0.018\n",
      "[290,   200] loss: 0.018\n",
      "[291,   100] loss: 0.018\n",
      "[291,   200] loss: 0.018\n",
      "[292,   100] loss: 0.018\n",
      "[292,   200] loss: 0.018\n",
      "[293,   100] loss: 0.018\n",
      "[293,   200] loss: 0.018\n",
      "[294,   100] loss: 0.018\n",
      "[294,   200] loss: 0.018\n",
      "[295,   100] loss: 0.018\n",
      "[295,   200] loss: 0.018\n",
      "[296,   100] loss: 0.018\n",
      "[296,   200] loss: 0.018\n",
      "[297,   100] loss: 0.018\n",
      "[297,   200] loss: 0.018\n",
      "[298,   100] loss: 0.018\n",
      "[298,   200] loss: 0.018\n",
      "[299,   100] loss: 0.018\n",
      "[299,   200] loss: 0.018\n",
      "[300,   100] loss: 0.018\n",
      "[300,   200] loss: 0.018\n",
      "[301,   100] loss: 0.018\n",
      "[301,   200] loss: 0.018\n",
      "[302,   100] loss: 0.018\n",
      "[302,   200] loss: 0.018\n",
      "[303,   100] loss: 0.018\n",
      "[303,   200] loss: 0.018\n",
      "[304,   100] loss: 0.018\n",
      "[304,   200] loss: 0.018\n",
      "[305,   100] loss: 0.018\n",
      "[305,   200] loss: 0.018\n",
      "[306,   100] loss: 0.017\n",
      "[306,   200] loss: 0.018\n",
      "[307,   100] loss: 0.018\n",
      "[307,   200] loss: 0.018\n",
      "[308,   100] loss: 0.018\n",
      "[308,   200] loss: 0.018\n",
      "[309,   100] loss: 0.018\n",
      "[309,   200] loss: 0.018\n",
      "[310,   100] loss: 0.018\n",
      "[310,   200] loss: 0.018\n",
      "[311,   100] loss: 0.018\n",
      "[311,   200] loss: 0.018\n",
      "[312,   100] loss: 0.018\n",
      "[312,   200] loss: 0.018\n",
      "[313,   100] loss: 0.018\n",
      "[313,   200] loss: 0.018\n",
      "[314,   100] loss: 0.018\n",
      "[314,   200] loss: 0.018\n",
      "[315,   100] loss: 0.018\n",
      "[315,   200] loss: 0.018\n",
      "[316,   100] loss: 0.018\n",
      "[316,   200] loss: 0.018\n",
      "[317,   100] loss: 0.018\n",
      "[317,   200] loss: 0.018\n",
      "[318,   100] loss: 0.018\n",
      "[318,   200] loss: 0.018\n",
      "[319,   100] loss: 0.018\n",
      "[319,   200] loss: 0.018\n",
      "[320,   100] loss: 0.017\n",
      "[320,   200] loss: 0.018\n",
      "[321,   100] loss: 0.017\n",
      "[321,   200] loss: 0.018\n",
      "[322,   100] loss: 0.018\n",
      "[322,   200] loss: 0.017\n",
      "[323,   100] loss: 0.018\n",
      "[323,   200] loss: 0.017\n",
      "[324,   100] loss: 0.018\n",
      "[324,   200] loss: 0.018\n",
      "[325,   100] loss: 0.018\n",
      "[325,   200] loss: 0.018\n",
      "[326,   100] loss: 0.017\n",
      "[326,   200] loss: 0.018\n",
      "[327,   100] loss: 0.018\n",
      "[327,   200] loss: 0.017\n",
      "[328,   100] loss: 0.018\n",
      "[328,   200] loss: 0.018\n",
      "[329,   100] loss: 0.018\n",
      "[329,   200] loss: 0.017\n",
      "[330,   100] loss: 0.018\n",
      "[330,   200] loss: 0.018\n",
      "[331,   100] loss: 0.018\n",
      "[331,   200] loss: 0.017\n",
      "[332,   100] loss: 0.018\n",
      "[332,   200] loss: 0.017\n",
      "[333,   100] loss: 0.018\n",
      "[333,   200] loss: 0.018\n",
      "[334,   100] loss: 0.018\n",
      "[334,   200] loss: 0.017\n",
      "[335,   100] loss: 0.017\n",
      "[335,   200] loss: 0.018\n",
      "[336,   100] loss: 0.017\n",
      "[336,   200] loss: 0.018\n",
      "[337,   100] loss: 0.017\n",
      "[337,   200] loss: 0.018\n",
      "[338,   100] loss: 0.018\n",
      "[338,   200] loss: 0.017\n",
      "[339,   100] loss: 0.018\n",
      "[339,   200] loss: 0.017\n",
      "[340,   100] loss: 0.017\n",
      "[340,   200] loss: 0.018\n",
      "[341,   100] loss: 0.018\n",
      "[341,   200] loss: 0.017\n",
      "[342,   100] loss: 0.018\n",
      "[342,   200] loss: 0.017\n",
      "[343,   100] loss: 0.018\n",
      "[343,   200] loss: 0.017\n",
      "[344,   100] loss: 0.017\n",
      "[344,   200] loss: 0.018\n",
      "[345,   100] loss: 0.017\n",
      "[345,   200] loss: 0.018\n",
      "[346,   100] loss: 0.017\n",
      "[346,   200] loss: 0.018\n",
      "[347,   100] loss: 0.017\n",
      "[347,   200] loss: 0.017\n",
      "[348,   100] loss: 0.018\n",
      "[348,   200] loss: 0.017\n",
      "[349,   100] loss: 0.017\n",
      "[349,   200] loss: 0.018\n",
      "[350,   100] loss: 0.017\n",
      "[350,   200] loss: 0.018\n",
      "[351,   100] loss: 0.017\n",
      "[351,   200] loss: 0.018\n",
      "[352,   100] loss: 0.017\n",
      "[352,   200] loss: 0.017\n",
      "[353,   100] loss: 0.017\n",
      "[353,   200] loss: 0.017\n",
      "[354,   100] loss: 0.017\n",
      "[354,   200] loss: 0.017\n",
      "[355,   100] loss: 0.017\n",
      "[355,   200] loss: 0.017\n",
      "[356,   100] loss: 0.018\n",
      "[356,   200] loss: 0.017\n",
      "[357,   100] loss: 0.017\n",
      "[357,   200] loss: 0.017\n",
      "[358,   100] loss: 0.017\n",
      "[358,   200] loss: 0.017\n",
      "[359,   100] loss: 0.017\n",
      "[359,   200] loss: 0.017\n",
      "[360,   100] loss: 0.017\n",
      "[360,   200] loss: 0.017\n",
      "[361,   100] loss: 0.018\n",
      "[361,   200] loss: 0.017\n",
      "[362,   100] loss: 0.017\n",
      "[362,   200] loss: 0.017\n",
      "[363,   100] loss: 0.017\n",
      "[363,   200] loss: 0.018\n",
      "[364,   100] loss: 0.017\n",
      "[364,   200] loss: 0.017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[365,   100] loss: 0.017\n",
      "[365,   200] loss: 0.017\n",
      "[366,   100] loss: 0.017\n",
      "[366,   200] loss: 0.017\n",
      "[367,   100] loss: 0.017\n",
      "[367,   200] loss: 0.017\n",
      "[368,   100] loss: 0.017\n",
      "[368,   200] loss: 0.017\n",
      "[369,   100] loss: 0.017\n",
      "[369,   200] loss: 0.017\n",
      "[370,   100] loss: 0.017\n",
      "[370,   200] loss: 0.017\n",
      "[371,   100] loss: 0.017\n",
      "[371,   200] loss: 0.017\n",
      "[372,   100] loss: 0.017\n",
      "[372,   200] loss: 0.018\n",
      "[373,   100] loss: 0.017\n",
      "[373,   200] loss: 0.018\n",
      "[374,   100] loss: 0.017\n",
      "[374,   200] loss: 0.017\n",
      "[375,   100] loss: 0.017\n",
      "[375,   200] loss: 0.017\n",
      "[376,   100] loss: 0.017\n",
      "[376,   200] loss: 0.017\n",
      "[377,   100] loss: 0.017\n",
      "[377,   200] loss: 0.017\n",
      "[378,   100] loss: 0.017\n",
      "[378,   200] loss: 0.017\n",
      "[379,   100] loss: 0.018\n",
      "[379,   200] loss: 0.017\n",
      "[380,   100] loss: 0.017\n",
      "[380,   200] loss: 0.017\n",
      "[381,   100] loss: 0.017\n",
      "[381,   200] loss: 0.017\n",
      "[382,   100] loss: 0.017\n",
      "[382,   200] loss: 0.017\n",
      "[383,   100] loss: 0.017\n",
      "[383,   200] loss: 0.017\n",
      "[384,   100] loss: 0.017\n",
      "[384,   200] loss: 0.017\n",
      "[385,   100] loss: 0.017\n",
      "[385,   200] loss: 0.017\n",
      "[386,   100] loss: 0.017\n",
      "[386,   200] loss: 0.017\n",
      "[387,   100] loss: 0.017\n",
      "[387,   200] loss: 0.017\n",
      "[388,   100] loss: 0.017\n",
      "[388,   200] loss: 0.017\n",
      "[389,   100] loss: 0.017\n",
      "[389,   200] loss: 0.017\n",
      "[390,   100] loss: 0.017\n",
      "[390,   200] loss: 0.017\n",
      "[391,   100] loss: 0.017\n",
      "[391,   200] loss: 0.017\n",
      "[392,   100] loss: 0.017\n",
      "[392,   200] loss: 0.017\n",
      "[393,   100] loss: 0.017\n",
      "[393,   200] loss: 0.017\n",
      "[394,   100] loss: 0.017\n",
      "[394,   200] loss: 0.017\n",
      "[395,   100] loss: 0.017\n",
      "[395,   200] loss: 0.017\n",
      "[396,   100] loss: 0.017\n",
      "[396,   200] loss: 0.017\n",
      "[397,   100] loss: 0.017\n",
      "[397,   200] loss: 0.017\n",
      "[398,   100] loss: 0.017\n",
      "[398,   200] loss: 0.017\n",
      "[399,   100] loss: 0.017\n",
      "[399,   200] loss: 0.017\n",
      "[400,   100] loss: 0.017\n",
      "[400,   200] loss: 0.017\n",
      "[401,   100] loss: 0.017\n",
      "[401,   200] loss: 0.017\n",
      "[402,   100] loss: 0.017\n",
      "[402,   200] loss: 0.017\n",
      "[403,   100] loss: 0.017\n",
      "[403,   200] loss: 0.017\n",
      "[404,   100] loss: 0.017\n",
      "[404,   200] loss: 0.017\n",
      "[405,   100] loss: 0.017\n",
      "[405,   200] loss: 0.017\n",
      "[406,   100] loss: 0.017\n",
      "[406,   200] loss: 0.017\n",
      "[407,   100] loss: 0.017\n",
      "[407,   200] loss: 0.017\n",
      "[408,   100] loss: 0.017\n",
      "[408,   200] loss: 0.017\n",
      "[409,   100] loss: 0.017\n",
      "[409,   200] loss: 0.017\n",
      "[410,   100] loss: 0.017\n",
      "[410,   200] loss: 0.017\n",
      "[411,   100] loss: 0.017\n",
      "[411,   200] loss: 0.017\n",
      "[412,   100] loss: 0.017\n",
      "[412,   200] loss: 0.017\n",
      "[413,   100] loss: 0.017\n",
      "[413,   200] loss: 0.017\n",
      "[414,   100] loss: 0.017\n",
      "[414,   200] loss: 0.017\n",
      "[415,   100] loss: 0.017\n",
      "[415,   200] loss: 0.017\n",
      "[416,   100] loss: 0.017\n",
      "[416,   200] loss: 0.017\n",
      "[417,   100] loss: 0.017\n",
      "[417,   200] loss: 0.017\n",
      "[418,   100] loss: 0.017\n",
      "[418,   200] loss: 0.017\n",
      "[419,   100] loss: 0.017\n",
      "[419,   200] loss: 0.017\n",
      "[420,   100] loss: 0.017\n",
      "[420,   200] loss: 0.017\n",
      "[421,   100] loss: 0.017\n",
      "[421,   200] loss: 0.017\n",
      "[422,   100] loss: 0.017\n",
      "[422,   200] loss: 0.017\n",
      "[423,   100] loss: 0.017\n",
      "[423,   200] loss: 0.017\n",
      "[424,   100] loss: 0.017\n",
      "[424,   200] loss: 0.017\n",
      "[425,   100] loss: 0.017\n",
      "[425,   200] loss: 0.017\n",
      "[426,   100] loss: 0.017\n",
      "[426,   200] loss: 0.017\n",
      "[427,   100] loss: 0.017\n",
      "[427,   200] loss: 0.017\n",
      "[428,   100] loss: 0.017\n",
      "[428,   200] loss: 0.017\n",
      "[429,   100] loss: 0.017\n",
      "[429,   200] loss: 0.017\n",
      "[430,   100] loss: 0.017\n",
      "[430,   200] loss: 0.017\n",
      "[431,   100] loss: 0.017\n",
      "[431,   200] loss: 0.017\n",
      "[432,   100] loss: 0.017\n",
      "[432,   200] loss: 0.017\n",
      "[433,   100] loss: 0.017\n",
      "[433,   200] loss: 0.017\n",
      "[434,   100] loss: 0.017\n",
      "[434,   200] loss: 0.017\n",
      "[435,   100] loss: 0.017\n",
      "[435,   200] loss: 0.017\n",
      "[436,   100] loss: 0.017\n",
      "[436,   200] loss: 0.017\n",
      "[437,   100] loss: 0.017\n",
      "[437,   200] loss: 0.017\n",
      "[438,   100] loss: 0.017\n",
      "[438,   200] loss: 0.017\n",
      "[439,   100] loss: 0.017\n",
      "[439,   200] loss: 0.017\n",
      "[440,   100] loss: 0.017\n",
      "[440,   200] loss: 0.017\n",
      "[441,   100] loss: 0.017\n",
      "[441,   200] loss: 0.017\n",
      "[442,   100] loss: 0.017\n",
      "[442,   200] loss: 0.017\n",
      "[443,   100] loss: 0.017\n",
      "[443,   200] loss: 0.017\n",
      "[444,   100] loss: 0.017\n",
      "[444,   200] loss: 0.017\n",
      "[445,   100] loss: 0.017\n",
      "[445,   200] loss: 0.017\n",
      "[446,   100] loss: 0.017\n",
      "[446,   200] loss: 0.017\n",
      "[447,   100] loss: 0.017\n",
      "[447,   200] loss: 0.017\n",
      "[448,   100] loss: 0.017\n",
      "[448,   200] loss: 0.017\n",
      "[449,   100] loss: 0.017\n",
      "[449,   200] loss: 0.017\n",
      "[450,   100] loss: 0.017\n",
      "[450,   200] loss: 0.017\n",
      "[451,   100] loss: 0.017\n",
      "[451,   200] loss: 0.017\n",
      "[452,   100] loss: 0.017\n",
      "[452,   200] loss: 0.017\n",
      "[453,   100] loss: 0.017\n",
      "[453,   200] loss: 0.017\n",
      "[454,   100] loss: 0.017\n",
      "[454,   200] loss: 0.017\n",
      "[455,   100] loss: 0.017\n",
      "[455,   200] loss: 0.017\n",
      "[456,   100] loss: 0.017\n",
      "[456,   200] loss: 0.017\n",
      "[457,   100] loss: 0.017\n",
      "[457,   200] loss: 0.017\n",
      "[458,   100] loss: 0.017\n",
      "[458,   200] loss: 0.017\n",
      "[459,   100] loss: 0.017\n",
      "[459,   200] loss: 0.017\n",
      "[460,   100] loss: 0.017\n",
      "[460,   200] loss: 0.017\n",
      "[461,   100] loss: 0.017\n",
      "[461,   200] loss: 0.017\n",
      "[462,   100] loss: 0.017\n",
      "[462,   200] loss: 0.017\n",
      "[463,   100] loss: 0.017\n",
      "[463,   200] loss: 0.017\n",
      "[464,   100] loss: 0.017\n",
      "[464,   200] loss: 0.017\n",
      "[465,   100] loss: 0.017\n",
      "[465,   200] loss: 0.017\n",
      "[466,   100] loss: 0.017\n",
      "[466,   200] loss: 0.017\n",
      "[467,   100] loss: 0.017\n",
      "[467,   200] loss: 0.017\n",
      "[468,   100] loss: 0.017\n",
      "[468,   200] loss: 0.017\n",
      "[469,   100] loss: 0.017\n",
      "[469,   200] loss: 0.017\n",
      "[470,   100] loss: 0.017\n",
      "[470,   200] loss: 0.017\n",
      "[471,   100] loss: 0.017\n",
      "[471,   200] loss: 0.017\n",
      "[472,   100] loss: 0.017\n",
      "[472,   200] loss: 0.017\n",
      "[473,   100] loss: 0.017\n",
      "[473,   200] loss: 0.017\n",
      "[474,   100] loss: 0.017\n",
      "[474,   200] loss: 0.017\n",
      "[475,   100] loss: 0.017\n",
      "[475,   200] loss: 0.017\n",
      "[476,   100] loss: 0.017\n",
      "[476,   200] loss: 0.017\n",
      "[477,   100] loss: 0.017\n",
      "[477,   200] loss: 0.017\n",
      "[478,   100] loss: 0.017\n",
      "[478,   200] loss: 0.017\n",
      "[479,   100] loss: 0.017\n",
      "[479,   200] loss: 0.017\n",
      "[480,   100] loss: 0.017\n",
      "[480,   200] loss: 0.017\n",
      "[481,   100] loss: 0.017\n",
      "[481,   200] loss: 0.017\n",
      "[482,   100] loss: 0.017\n",
      "[482,   200] loss: 0.017\n",
      "[483,   100] loss: 0.017\n",
      "[483,   200] loss: 0.017\n",
      "[484,   100] loss: 0.017\n",
      "[484,   200] loss: 0.017\n",
      "[485,   100] loss: 0.017\n",
      "[485,   200] loss: 0.017\n",
      "[486,   100] loss: 0.017\n",
      "[486,   200] loss: 0.017\n",
      "[487,   100] loss: 0.017\n",
      "[487,   200] loss: 0.017\n",
      "[488,   100] loss: 0.017\n",
      "[488,   200] loss: 0.017\n",
      "[489,   100] loss: 0.017\n",
      "[489,   200] loss: 0.017\n",
      "[490,   100] loss: 0.017\n",
      "[490,   200] loss: 0.017\n",
      "[491,   100] loss: 0.017\n",
      "[491,   200] loss: 0.017\n",
      "[492,   100] loss: 0.017\n",
      "[492,   200] loss: 0.017\n",
      "[493,   100] loss: 0.017\n",
      "[493,   200] loss: 0.017\n",
      "[494,   100] loss: 0.017\n",
      "[494,   200] loss: 0.017\n",
      "[495,   100] loss: 0.017\n",
      "[495,   200] loss: 0.017\n",
      "[496,   100] loss: 0.017\n",
      "[496,   200] loss: 0.017\n",
      "[497,   100] loss: 0.017\n",
      "[497,   200] loss: 0.017\n",
      "[498,   100] loss: 0.017\n",
      "[498,   200] loss: 0.017\n",
      "[499,   100] loss: 0.017\n",
      "[499,   200] loss: 0.017\n",
      "[500,   100] loss: 0.017\n",
      "[500,   200] loss: 0.017\n"
     ]
    }
   ],
   "source": [
    "## Training auto encoder and dynamics network\n",
    "resume_previous_training = True\n",
    "epochs = 300\n",
    "if not resume_previous_training:\n",
    "    prev_offset = 0\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-4)\n",
    "writer = SummaryWriter('runs/' + expt_name)\n",
    "\n",
    "running_loss = np.array([0.0, 0.0, 0.0, 0.0])\n",
    "for epoch in range(epochs):\n",
    "    for i, data in enumerate(training_loader, 0):\n",
    "        data = data.float()\n",
    "        ip_batch = data[:, x_dim:]\n",
    "        # Divide by 255 to scale the input to bring between 0 and 1\n",
    "        ip_batch = ip_batch / 255\n",
    "        op_batch = net(ip_batch)\n",
    "        total_loss = F.mse_loss(op_batch, ip_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += total_loss.item()\n",
    "        if i % 100 == 99:\n",
    "            avg_loss = running_loss / 100\n",
    "            print ('[%d, %5d] loss: %.3f' % (prev_offset+epoch+1, i+1, avg_loss[0]))\n",
    "            index = (prev_offset+epoch) * len(training_loader) + i\n",
    "            writer.add_scalar('training_loss', avg_loss[0], index)\n",
    "            running_loss[:] = 0.0\n",
    "\n",
    "prev_offset += epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.011260881088674068\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_data = iter(test_loader).next()\n",
    "#     test_img = np.random.randint(len(test_data))\n",
    "#     data = torch.as_tensor(test_data[test_img].reshape(1,-1))\n",
    "    test_data = test_data.float()\n",
    "    ip = test_data[:, x_dim:] / 255\n",
    "    op_n = net(ip)\n",
    "    op = torch.floor(op_n * 255) # Post processing to match the pre-processing of inputs in network\n",
    "    rows = 3 # test_batch size\n",
    "    \n",
    "    print ('MSE: {}'.format(np.mean(np.abs(ip.numpy() - op_n.numpy()) ** 2)))\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    \n",
    "    for r in range(rows):\n",
    "        ax = plt.subplot(rows,2,r*2 + 1)\n",
    "        plt.imshow(ip[r].reshape(img_res, img_res))\n",
    "        if r==0:\n",
    "            ax.title.set_text('Original')\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "        ax = plt.subplot(rows,2,r*2 + 2)\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "        plt.imshow(op[r].reshape(img_res, img_res))\n",
    "        if r==0:\n",
    "            ax.title.set_text('Recontructed')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print (ip[0].numpy())\n",
    "print (np.array(op_n[0].numpy(), dtype=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(**opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(folder)\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_net = AutoEncoder()\n",
    "test_net.load_state_dict(torch.load(PATH))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demopl",
   "language": "python",
   "name": "demopl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
